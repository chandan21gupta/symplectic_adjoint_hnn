{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "db402daa-45e5-4f91-b2ff-2b6b7c92402a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import gradcheck\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a3fe01df-9bff-4bed-b825-9353a4eb3d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HarmonicOscillatorDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Load the dataset\n",
    "        dataset = np.load(file_path)[:, ::1000, :]\n",
    "\n",
    "        # Convert the dataset to PyTorch tensors\n",
    "        self.p_values = torch.tensor(dataset[:, :, 0], dtype=torch.float32)\n",
    "        self.q_values = torch.tensor(dataset[:, :, 1], dtype=torch.float32)\n",
    "        self.h_values = torch.tensor(dataset[:, :, 2], dtype=torch.float32)\n",
    "\n",
    "        # Ensure consistent length for all tensors\n",
    "        assert len(self.p_values) == len(self.q_values) == len(self.h_values)\n",
    "        self.length = len(self.p_values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.p_values[idx], self.q_values[idx], self.h_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "d13d18c1-3fe9-495d-9ea3-084c3228b2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_General_Hamilt(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(MLP_General_Hamilt, self).__init__()\n",
    "        self.linear1 = nn.Linear(2*n_input, n_hidden)\n",
    "        self.linear2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear4 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear5 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear6 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, p, q):\n",
    "        pq = torch.cat((p, q), 1)\n",
    "        h = self.linear1(pq)\n",
    "        h = h.sigmoid_()\n",
    "        h = self.linear2(h)\n",
    "        h = h.sigmoid_()\n",
    "        h = self.linear3(h)\n",
    "        h = h.sigmoid_()\n",
    "        h = self.linear4(h)\n",
    "        h = h.sigmoid_()\n",
    "        h = self.linear5(h)\n",
    "        h = h.sigmoid_()\n",
    "        h = self.linear6(h)\n",
    "        return h\n",
    "\n",
    "class MLP2H_Separable_Hamilt(nn.Module):\n",
    "    def __init__(self, n_hidden, input_size):\n",
    "        super(MLP2H_Separable_Hamilt, self).__init__()\n",
    "        self.linear_K1 = nn.Linear(input_size, n_hidden)\n",
    "        self.linear_K1B = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_K2 = nn.Linear(n_hidden, 1)\n",
    "        self.linear_P1 = nn.Linear(input_size, n_hidden)\n",
    "        self.linear_P1B = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_P2 = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def kinetic_energy(self, p):\n",
    "        h_pre = self.linear_K1(p)\n",
    "        h = h_pre.tanh_()\n",
    "        # h = h_pre.sigmoid()\n",
    "        h_pre_B = self.linear_K1B(h)\n",
    "        h_B = h_pre_B.tanh_()\n",
    "        # h_B = h_pre_B.sigmoid()\n",
    "        return self.linear_K2(h_B)\n",
    "\n",
    "    def potential_energy(self, q):\n",
    "        h_pre = self.linear_P1(q)\n",
    "        h = h_pre.tanh_()\n",
    "        h_pre_B = self.linear_P1B(h)\n",
    "        h_B = h_pre_B.tanh_()\n",
    "        return self.linear_P2(h_B)\n",
    "\n",
    "    def forward(self, p, q):\n",
    "        return self.kinetic_energy(p) + self.potential_energy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "10b33555-7d05-4e90-99d8-f15979c96fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BackProp_HNN(nn.Module):\n",
    "    def __init__(self, f, T, dt, dim, integrator, iter):\n",
    "        super(BackProp_HNN, self).__init__()\n",
    "        self.f = f\n",
    "        self.dt = dt\n",
    "        self.dim = dim\n",
    "        self.integrator = integrator\n",
    "        self.T = T\n",
    "        self.num_steps = int(1/dt)\n",
    "        self.iter = iter\n",
    "        \n",
    "    def forward(self, p0, q0):\n",
    "        trajectories = torch.zeros((p0.shape[0], self.T, self.dim * 2)).to(device)\n",
    "        trajectories[:, 0, 0] = p0\n",
    "        trajectories[:, 0, 1] = q0\n",
    "        p = p0\n",
    "        q = q0\n",
    "        for timestep in range(1, T):\n",
    "            for _ in range(self.num_steps):\n",
    "                if self.integrator == \"euler\":\n",
    "                    x = self.euler_step(torch.stack([p, q], dim = 1))\n",
    "                elif self.integrator == \"rk2\":\n",
    "                    x = self.rk2_step(torch.stack([p, q], dim = 1))\n",
    "                elif self.integrator == \"sv\":\n",
    "                    x = self.sv_step(x = torch.stack([p, q], dim = 1), iterations = iter)\n",
    "                elif self.integrator == \"pc\":\n",
    "                    x = self.pc_step(x = torch.stack([p, q], dim = 1), iterations = iter)\n",
    "                # print(x.shape)\n",
    "                p = x[:, 0]\n",
    "                q = x[:, 1]\n",
    "            trajectories[:, timestep, 0] = p\n",
    "            trajectories[:, timestep, 1] = q\n",
    "        return trajectories\n",
    "    \n",
    "    def euler_step(self, x):\n",
    "        return x + self.dt * self.dynamics_fn(x)\n",
    "    \n",
    "    def leapfrog_step(self, x):\n",
    "        p_ = x[:, 0] + 0.5 * self.dt * self.dynamics_fn(x)[:, 0]\n",
    "        q = x[:, 1] + self.dt * self.dynamics_fn(torch.stack([p_, x[:, 1]], dim = 1))[:, 1]\n",
    "        p = p_ + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_, q], dim = 1))[:, 0]\n",
    "        return torch.stack([p, q], dim = 1)\n",
    "\n",
    "    def rk2_step(self, x):\n",
    "        p0 = x[:,0]\n",
    "        q0 = x[:,1]\n",
    "        out = self.dynamics_fn(x)\n",
    "        p1 = out[:, 0]\n",
    "        q1 = out[:, 1]\n",
    "\n",
    "        out = self.dynamics_fn(torch.stack([p0 + 0.5 * self.dt * p1, q0 + 0.5 * self.dt * q1], dim = 1))\n",
    "        p2 = out[:, 0]\n",
    "        q2 = out[:, 1]\n",
    "\n",
    "        p = p0 + self.dt * (p1 + p2)/2\n",
    "        q = q0 + self.dt * (q1 + q2)/2\n",
    "        return torch.stack([p, q], dim = 1)\n",
    "\n",
    "\n",
    "    def sv_step(self, x, x_init = None, iterations = 1):\n",
    "        p0 = x[:, 0]\n",
    "        q0 = x[:, 1]\n",
    "        if x_init == None:\n",
    "            p_half = p0\n",
    "        else:\n",
    "            p_half = (x_init[:, 0] + p0)/2\n",
    "        for _ in range(iterations):\n",
    "            p_half = p0 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q0], dim = 1))[:, 0]\n",
    "        q1 = q0 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q0], dim = 1))[:, 1]\n",
    "        if x_init == None:\n",
    "            q2 = q0\n",
    "        else:\n",
    "            q2 = (x_init[:, 1] + q0)/2\n",
    "        for _ in range(iterations):\n",
    "            q2 = q1 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q2], dim = 1))[:, 1]\n",
    "        p1 = p_half + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q2], dim = 1))[:, 0]\n",
    "        return torch.stack([p1, q2], dim = 1)\n",
    "\n",
    "    def pc_step(self, x, iterations = 1):\n",
    "        out = self.rk2_step(x)\n",
    "        out = self.sv_step(x, out, iterations)\n",
    "        return out\n",
    "            \n",
    "    def dynamics_fn(self, x):\n",
    "        p = x[:, 0]\n",
    "        q = x[:, 1]\n",
    "        p.requires_grad_(True)\n",
    "        q.requires_grad_(True)\n",
    "        # h = self.f(p, q)\n",
    "        h = self.f(p.unsqueeze(-1), q.unsqueeze(-1))\n",
    "        # print(\"dynamics_fn\", h.shape)\n",
    "        grad_p, = grad(h.sum(), p, create_graph=True, allow_unused=True)\n",
    "        grad_q, = grad(h.sum(), q, create_graph=True, allow_unused=True)\n",
    "        return torch.stack([-grad_q, grad_p], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cab7c-01be-43e4-9c3d-0651a96d94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackProp_HNN(nn.Module):\n",
    "    def __init__(self, f, T, dt, dim, integrator, iter):\n",
    "        super(BackProp_HNN, self).__init__()\n",
    "        self.f = f\n",
    "        self.dt = dt\n",
    "        self.dim = dim\n",
    "        self.integrator = integrator\n",
    "        self.T = T\n",
    "        self.num_steps = int(1/dt)\n",
    "        self.iter = iter\n",
    "\n",
    "    def forward(self, p0, q0):\n",
    "        trajectories = torch.zeros((p0.shape[0], self.T, self.dim * 2)).to(device)\n",
    "        trajectories[:, 0, 0] = p0\n",
    "        trajectories[:, 0, 1] = q0\n",
    "        p = p0\n",
    "        q = q0\n",
    "        for timestep in range(1, T):\n",
    "            for _ in range(self.num_steps):\n",
    "                if self.integrator == \"euler\":\n",
    "                    x = self.euler_step(torch.stack([p, q], dim = 1))\n",
    "                elif self.integrator == \"rk2\":\n",
    "                    x = self.rk2_step(torch.stack([p, q], dim = 1))\n",
    "                elif self.integrator == \"sv\":\n",
    "                    x = self.sv_step(torch.stack([p, q], dim = 1), x_init = None, iterations = iter)\n",
    "                elif self.integrator == \"pc\":\n",
    "                    x = self.rk2_step(torch.stack([p, q], dim = 1))\n",
    "                    x = self.sv_step(torch.stack([p, q], dim = 1), x_init = x, iterations = iter)\n",
    "                    # x = self.pc_step(torch.stack([p, q], dim = 1), iterations = iter)\n",
    "                # print(x.shape)\n",
    "                p = x[:, 0]\n",
    "                q = x[:, 1]\n",
    "            trajectories[:, timestep, 0] = p\n",
    "            trajectories[:, timestep, 1] = q\n",
    "        return trajectories\n",
    "\n",
    "    def euler_step(self, x):\n",
    "        return x + self.dt * self.dynamics_fn(x)\n",
    "\n",
    "    def rk2_step(self, x):\n",
    "        p0 = x[:,0]\n",
    "        q0 = x[:,1]\n",
    "        out = self.dynamics_fn(x)\n",
    "        p1 = out[:, 0]\n",
    "        q1 = out[:, 1]\n",
    "\n",
    "        out = self.dynamics_fn(torch.stack([p0 + 0.5 * self.dt * p1, q0 + 0.5 * self.dt * q1], dim = 1))\n",
    "        p2 = out[:, 0]\n",
    "        q2 = out[:, 1]\n",
    "\n",
    "        p = p0 + self.dt * (p1 + p2)/2\n",
    "        q = q0 + self.dt * (q1 + q2)/2\n",
    "        return torch.stack([p, q], dim = 1)\n",
    "\n",
    "\n",
    "    def sv_step(self, x, x_init = None, iterations = 1):\n",
    "        p0 = x[:, 0]\n",
    "        q0 = x[:, 1]\n",
    "        if x_init == None:\n",
    "            p_half = p0\n",
    "        else:\n",
    "            p_half = (x_init[:, 0] + p0)/2\n",
    "        for _ in range(iterations):\n",
    "            p_half = p0 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q0], dim = 1))[:, 0]\n",
    "        q1 = q0 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q0], dim = 1))[:, 1]\n",
    "        if x_init == None:\n",
    "            q2 = q1\n",
    "        else:\n",
    "            q2 = (x_init[:, 1] + q1)/2\n",
    "        for _ in range(iterations):\n",
    "            q2 = q1 + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q2], dim = 1))[:, 1]\n",
    "        p1 = p_half + 0.5 * self.dt * self.dynamics_fn(torch.stack([p_half, q2], dim = 1))[:, 0]\n",
    "        return torch.stack([p1, q2], dim = 1)\n",
    "\n",
    "    # def pc_step(self, x, iterations = 1):\n",
    "        # out = self.rk2_step(x)\n",
    "        # out = self.sv_step(x, out, iterations)\n",
    "        # return self.rk2_step(x)\n",
    "\n",
    "    def dynamics_fn(self, x):\n",
    "        p = x[:, 0]\n",
    "        q = x[:, 1]\n",
    "        p.requires_grad_(True)\n",
    "        q.requires_grad_(True)\n",
    "        # h = self.f(p, q)\n",
    "        h = self.f(p.unsqueeze(-1), q.unsqueeze(-1))\n",
    "        # print(\"dynamics_fn\", h.shape)\n",
    "        grad_p, = grad(h.sum(), p, create_graph=True, allow_unused=True)\n",
    "        grad_q, = grad(h.sum(), q, create_graph=True, allow_unused=True)\n",
    "        return torch.stack([-grad_q, grad_p], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ed3ea-5536-45bb-b956-6165aa18cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for integrator in [\"euler\", \"rk2\", \"sv\", \"pc\"]:\n",
    "\n",
    "    f1 = MLP_General_Hamilt(n_input = 1, n_hidden = 64)\n",
    "    # f1 = MLP2H_Separable_Hamilt(n_hidden=256, input_size=1).to(device).double()\n",
    "    T = 2\n",
    "    dt = 0.1\n",
    "    dim = 1\n",
    "    num_epochs = 100\n",
    "    iter = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create the model, criterion, optimizer, and data loaders\n",
    "    model = BackProp_HNN(f1, T, dt, dim, integrator, iter).to(device).double()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "    train_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/train.npy\")\n",
    "    data_loader = DataLoader(train_mass_spring, batch_size=32, shuffle=False)\n",
    "    val_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/val.npy\")\n",
    "    val_loader = DataLoader(val_mass_spring, batch_size=32, shuffle=False)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = integrator+str(dt)+'_best_model.pt'\n",
    "\n",
    "    with open(integrator+\"_loss_\"+str(dt)+\".txt\", \"w\") as loss_file:\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            loss_epoch = []\n",
    "            for batch in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                p_batch, q_batch, _ = batch\n",
    "                p0_batch = p_batch[:, 0].to(device).double()\n",
    "                q0_batch = q_batch[:, 1].to(device).double()\n",
    "                simulated_trajectory = model(p0_batch, q0_batch)\n",
    "                simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "                trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "                trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "                loss = criterion(trajectory, simulated_trajectory)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loss_epoch.append(loss.item())\n",
    "            avg_train_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "            train_loss.append(avg_train_loss)\n",
    "\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            val_loss_epoch = []\n",
    "            for batch in val_loader:\n",
    "                optimizer.zero_grad()\n",
    "                p_batch, q_batch, _ = batch\n",
    "                p0_batch = p_batch[:, 0].to(device).double()\n",
    "                q0_batch = q_batch[:, 1].to(device).double()\n",
    "                simulated_trajectory = model(p0_batch, q0_batch)\n",
    "                simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "                trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "                trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "                loss = criterion(trajectory, simulated_trajectory)\n",
    "                val_loss_epoch.append(loss.item())\n",
    "            avg_val_loss = sum(val_loss_epoch)/len(val_loss_epoch)\n",
    "            val_loss.append(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "            loss_file.write(f\"{avg_train_loss}, {avg_val_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54824a4c-cb0f-4d6e-93ae-edb7526fdbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hamiltonian_Adjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, p0, q0, f, T, dt, back_dt, dim, integrator, back_integrator, iter, *adjoint_params):\n",
    "        with torch.no_grad():\n",
    "            trajectories = torch.zeros((p0.shape[0], T, dim * 2))\n",
    "            trajectories[:, 0, 0] = p0\n",
    "            trajectories[:, 0, 1] = q0\n",
    "            p = p0\n",
    "            q = q0\n",
    "            num_steps = int(1/dt)\n",
    "            if integrator == \"euler\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.euler_step\n",
    "            elif integrator == \"rk2\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.rk2_step\n",
    "            elif integrator == \"sv\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.sv_step                \n",
    "            # elif integrator == \"rk2\":\n",
    "            #     integrate_fn = Hamiltonian_Adjoint.rk2\n",
    "            # elif integrate_fn == \"sv\":\n",
    "            #     integrate_fn = Hamiltonian_Adjoint.sv\n",
    "            dynamics = Hamiltonian_Adjoint.dynamics_fn\n",
    "            for timestep in range(1, T):\n",
    "                for _ in range(num_steps):\n",
    "                    x = integrate_fn([torch.stack([p, q], dim = 1)], dynamics, f, dt)[0]\n",
    "                    # if integrator == \"predictor_corrector\":\n",
    "                    #     x = integrate_fn([torch.stack([p, q], dim = 1)], dynamics, None, f, dt)[0]\n",
    "                    # else:\n",
    "                    #     x = integrate_fn([torch.stack([p, q], dim = 1)], dynamics, f, dt)[0]\n",
    "                    p = x[:, 0]\n",
    "                    q = x[:, 1]\n",
    "                trajectories[:, timestep, 0] = p\n",
    "                trajectories[:, timestep, 1] = q\n",
    "            ctx.save_for_backward(trajectories, p0, q0, *adjoint_params)\n",
    "            ctx.T = T\n",
    "            ctx.dt = dt\n",
    "            ctx.dim = dim\n",
    "            ctx.back_integrator = back_integrator\n",
    "            ctx.iter = iter\n",
    "            ctx.back_dt = back_dt\n",
    "            ctx.f = f\n",
    "            # print(\"trajectories p\", trajectories[0, : , 0])\n",
    "            # return trajectories[:, :, 0], trajectories[:, :, 1]\n",
    "            return trajectories\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dldz):\n",
    "        with torch.no_grad():\n",
    "            dldp = dldz[:, :, 0]\n",
    "            dldq = dldz[:, :, 1]\n",
    "            # print(\"dldp\", dldp.shape)\n",
    "            # print(\"dldq\", dldq.shape)\n",
    "            trajectories, p0, q0, *adjoint_params = ctx.saved_tensors\n",
    "            T = ctx.T\n",
    "            dt = ctx.dt\n",
    "            dim = ctx.dim\n",
    "            back_integrator = ctx.back_integrator\n",
    "            back_dt = ctx.back_dt\n",
    "            iter = ctx.iter\n",
    "            if back_integrator == \"euler\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.euler_step\n",
    "            elif back_integrator == \"rk2\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.rk2_step\n",
    "            elif back_integrator == \"sv\":\n",
    "                integrate_fn = Hamiltonian_Adjoint.sv_step\n",
    "            # elif back_integrator == \"second_order\":\n",
    "            #     integrate_fn = Hamiltonian_Adjoint.heuns_method\n",
    "            # elif back_integrator == \"predictor_corrector\":\n",
    "            #     integrate_fn = Hamiltonian_Adjoint.predictor_corrector\n",
    "            num_steps = int(1/back_dt)\n",
    "            h = T/num_steps\n",
    "            f = ctx.f\n",
    "\n",
    "            def augmented_dynamics(y_aug, Func):\n",
    "                #get p and q at current time step\n",
    "                p = y_aug[0]\n",
    "                q = y_aug[1]\n",
    "                with torch.enable_grad():\n",
    "                    p = p.detach().requires_grad_(True).double()\n",
    "                    q = q.detach().requires_grad_(True).double()\n",
    "                    H = Func(p.unsqueeze(-1), q.unsqueeze(-1))\n",
    "                    # following three lines are necessary to bypass a bug in pytorch's autograd function, and do not contribute the actual algorithm whatsoever\n",
    "                    _p = torch.as_strided(p, (), ())  # noqa\n",
    "                    _q = torch.as_strided(q, (), ())  # noqa\n",
    "                    _params = tuple(torch.as_strided(param, (), ()) for param in adjoint_params)\n",
    "                    #partial gradient of Hamiltonian with respect to p and q\n",
    "                    dhdp, = torch.autograd.grad(H.sum(), p, allow_unused = True, retain_graph = True, create_graph = True)\n",
    "                    dhdq, = torch.autograd.grad(H.sum(), q, allow_unused = True, retain_graph = True, create_graph = True)\n",
    "                    #partial gradient of Hamiltonian with p and q, the vector jacobian product being -adjoint_q, -lambda_q*dh/dpdq\n",
    "                    dhdpdq_1 = torch.autograd.grad(dhdp, q, -y_aug[3], allow_unused=True, retain_graph=True)\n",
    "                    # print(\"dhdpdq_1\", dhdpdq_1[0])\n",
    "                    #partial gradient of Hamiltonian with p and q, the vector jacobian product being adjoint_p, lambda_p*dh/dpdq\n",
    "                    dhdpdq_2 = torch.autograd.grad(dhdp, q, y_aug[2], allow_unused=True, retain_graph=True)\n",
    "                    # print(\"dhdpdq_2\", dhdpdq_2[0])\n",
    "                    #double partial gradients with respect to p and q adjusted by respective jacobian products as in the formula\n",
    "                    #-lambda_q*dh/dpdp\n",
    "                    dhdpdp = torch.autograd.grad(dhdp, p, -y_aug[3], allow_unused=True, retain_graph=True)\n",
    "                    # print(\"dhdpdp\", dhdpdp[0][:5])\n",
    "                    #lambda_p*dh/dqdq\n",
    "                    dhdqdq = torch.autograd.grad(dhdq, q, y_aug[2], allow_unused=True, retain_graph=True)\n",
    "                    # print(\"dhdqdq\", dhdqdq[0][:5])\n",
    "                    #partial gradients with respect to parameters, p and q adjusted with respective adjoints as given in the formula\n",
    "                    # -lambda_q*dh/dpdtheta\n",
    "                    dhdpdw = torch.autograd.grad(dhdp, adjoint_params, y_aug[3], allow_unused=True, retain_graph=True)\n",
    "                    #lambda_p*dh/dqdtheta\n",
    "                    dhdqdw = torch.autograd.grad(dhdq, adjoint_params, -y_aug[2], allow_unused=True, retain_graph=True)\n",
    "                    dhdpdq_1 = [torch.zeros_like(p) if param is None else param for param in dhdpdq_1]\n",
    "                    dhdpdq_2 = [torch.zeros_like(q) if param is None else param for param in dhdpdq_2]\n",
    "                    #setting gradients zero for parameters which may not directly contribute to Hamiltonian calculation at different time steps\n",
    "                    dhdpdp = torch.zeros_like(p) if dhdpdp is None else dhdpdp\n",
    "                    dhdqdq = torch.zeros_like(q) if dhdqdq is None else dhdqdq\n",
    "                    dhdpdw = [torch.zeros_like(param) if vjp_param is None else vjp_param\n",
    "                              for param, vjp_param in zip(adjoint_params, dhdpdw)]\n",
    "                    dhdqdw = [torch.zeros_like(param) if vjp_param is None else vjp_param\n",
    "                              for param, vjp_param in zip(adjoint_params, dhdqdw)]\n",
    "                    #final gradient calculation (lambda_p*dh/dqdtheta - lambda_q*dh/dpdtheta)\n",
    "                    dw = list(dhdp_param + dhdq_param for dhdp_param, dhdq_param in zip(dhdpdw, dhdqdw))\n",
    "                    f_p = -dhdq\n",
    "                    f_q = dhdp\n",
    "                    #lambda_p = -lambda_q*dh/dpdq + lambda_p*dh/dqdq\n",
    "                    adjoint_p = dhdpdq_2[0] + dhdpdp[0]\n",
    "                    #lambda_q = -lambda_q*dh/dpdp + lambda_p*dh/dpdq\n",
    "                    adjoint_q = dhdqdq[0] + dhdpdq_1[0]\n",
    "                    # return [f_p, f_q, adjoint_p, adjoint_q]\n",
    "                    return [f_p, f_q, adjoint_p, adjoint_q, *dw]\n",
    "            adj_p = torch.zeros(dldp.shape[0]).to(dldp)\n",
    "            adj_q = torch.zeros(dldq.shape[0]).to(dldq)\n",
    "            adj_params = [torch.zeros_like(param).to(dldp) for param in adjoint_params]\n",
    "            final_aug = None\n",
    "            for i in range(T-1, 0, -1):\n",
    "                p_next = trajectories[:, i, 0]\n",
    "                q_next = trajectories[:, i, 1]\n",
    "                adj_p += dldp[:, i]\n",
    "                adj_q += dldq[:, i]\n",
    "                aug_state = [p_next, q_next, adj_p, adj_q]\n",
    "                aug_state.extend([torch.zeros_like(param).to(dldp) for param in adjoint_params])\n",
    "                for _ in range(num_steps):\n",
    "                    aug_state = integrate_fn(aug_state, augmented_dynamics, f, back_dt)\n",
    "                    # if back_integrator == 'predictor_corrector':\n",
    "                    #     aug_state = integrate_fn(aug_state, augmented_dynamics, f, back_dt, iter)\n",
    "                    # else:\n",
    "                    #     aug_state = integrate_fn(aug_state, augmented_dynamics, f, back_dt)\n",
    "                    if not final_aug:\n",
    "                        final_aug = aug_state[4:]\n",
    "                        aug_state = aug_state[:4]\n",
    "                        aug_state.extend([torch.zeros_like(param).to(dldp) for param in adjoint_params])\n",
    "                    else:\n",
    "                        adj_params = [val1 + val2 for val1, val2 in zip(adj_params, aug_state[4:])]\n",
    "                        aug_state = aug_state[:4]\n",
    "                        aug_state.extend([torch.zeros_like(param).to(dldp) for param in adjoint_params])\n",
    "                adj_p = aug_state[2]\n",
    "                adj_q = aug_state[3]\n",
    "                # print(\"param\", aug_state[4][:5])\n",
    "                # adj_params = [val1 + val2 for val1, val2 in zip(adj_params, aug_state[4:])]\n",
    "\n",
    "            adj_p += dldp[:, 0]\n",
    "            adj_q += dldq[:, 0]\n",
    "            p_next = trajectories[:, 0, 0]\n",
    "            q_next = trajectories[:, 0, 1]\n",
    "            aug_state = [p_next, q_next, adj_p, adj_q]\n",
    "            aug_state.extend([torch.zeros_like(param).to(dldp) for param in adjoint_params])\n",
    "            return_aug = augmented_dynamics(aug_state, f)\n",
    "            avg = [0.5 * (val1 + val2) for val1, val2 in zip(final_aug, return_aug[4:])]\n",
    "            adj_params = [val1 + val2 for val1, val2 in zip(avg, adj_params)]\n",
    "            adj_params = [h * val for param, val in zip(adjoint_params, adj_params)]\n",
    "            # adj_params = aug_state[4:]\n",
    "            # print(\"adj_q\", adj_q[:32])\n",
    "\n",
    "        return adj_p, adj_q, None, None, None, None, None, None, None, None, *adj_params\n",
    "\n",
    "    @staticmethod\n",
    "    def euler_step(x, dynamics, f, dt):\n",
    "        if len(x) > 2:\n",
    "            func_out = dynamics(x, f)\n",
    "            out = func_out[:4]\n",
    "            result = [xi + dt * yi for xi, yi in zip(x, out)]\n",
    "            return result + func_out[4:]\n",
    "        out = dynamics(x, f)\n",
    "        result = [xi + dt * yi for xi, yi in zip(x, out)]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def sv_step(x, dynamics, f, dt, x_init = None, iterations = 1):\n",
    "        if len(x) > 2:\n",
    "            p0 = x[0]\n",
    "            q0 = x[1]\n",
    "            r0 = x[2]\n",
    "            s0 = x[3]\n",
    "            if x_init == None:\n",
    "                p_half = p0\n",
    "            else:\n",
    "                p_half = (x_init[0] + p0)/2\n",
    "            for _ in range(iterations):\n",
    "                p_half = p0 + 0.5 * dt * dynamics([p_half, q0, r0, s0], f)[0]\n",
    "            r1 = r0 + 0.5 * dynamics([p_half, q0, r0, s0], f)[2]\n",
    "            if x_init == None:\n",
    "                r2 = r1\n",
    "            else:\n",
    "                r2 = (x_init[2] + r1)/2\n",
    "            for _ in range(iterations):\n",
    "                r2 = r1 + 0.5 * dt * dynamics([p_half, q0, r2, s0], f)[2]\n",
    "            p1 = p_half + 0.5 * dynamics([p_half, q0, r2, s0], f)[0]\n",
    "            if x_init == None:\n",
    "                s_half = s0\n",
    "            else:\n",
    "                s_half = (x_init[3] + s0)/2\n",
    "            for _ in range(iterations):\n",
    "                s_half = s0 + 0.5 * dt * dynamics([p0, q0, r0, s_half], f)[3]\n",
    "            q1 = q0 + 0.5 * dynamics([p0, q0, r0, s_half], f)[1]\n",
    "            if x_init == None:\n",
    "                q2 = q1\n",
    "            else:\n",
    "                q2 = (x_init[1] + q1)/2\n",
    "            for _ in range(iterations):\n",
    "                q2 = q1 + 0.5 * dt * dynamics([p0, q2, r0, s_half], f)[1]\n",
    "            s1 = s_half + 0.5 * dt * dynamics([p0, q2, r0, s_half], f)[3]\n",
    "            return [p1, q2, r2, s1] + x[4:]\n",
    "\n",
    "        p0 = x[0][:, 0]\n",
    "        q0 = x[0][:, 1]\n",
    "        if x_init == None:\n",
    "            p_half = p0\n",
    "        else:\n",
    "            p_half = (x_init[0][:, 0] + p0)/2\n",
    "        for _ in range(iterations):\n",
    "            p_half = p0 + 0.5 * dt * dynamics([torch.stack([p_half, q0], dim = 1)], f)[0][:, 0]\n",
    "        q1 = q0 + 0.5 * dt * dynamics([torch.stack([p_half, q0], dim = 1)], f)[0][:, 1]\n",
    "        if x_init == None:\n",
    "            q2 = q1\n",
    "        else:\n",
    "            q2 = (x_init[0][:, 1] + q1)/2\n",
    "        for _ in range(iterations):\n",
    "            q2 = q1 + 0.5 * dt * dynamics([torch.stack([p_half, q2], dim = 1)], f)[0][:, 1]\n",
    "        p1 = p_half + 0.5 * dt * dynamics([torch.stack([p_half, q2], dim = 1)], f)[0][:, 0]\n",
    "        return [torch.stack([p1, q2], dim = 1)]\n",
    "\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def rk2_step(x, dynamics, f, dt):\n",
    "        if len(x) > 2:\n",
    "            p0 = x[0]\n",
    "            q0 = x[1]\n",
    "            r0 = x[2]\n",
    "            s0 = x[3]\n",
    "            func_out = dynamics(x, f)\n",
    "            out = func_out[:4]\n",
    "            p1 = out[0]\n",
    "            q1 = out[1]\n",
    "            r1 = out[2]\n",
    "            s1 = out[3]\n",
    "\n",
    "            func_out = dynamics([p0 + 0.5 * dt * p1, q0 + 0.5 * dt * q1, r0 + 0.5 * dt * r1, s0 + 0.5 * dt * s1], f)\n",
    "            out = func_out[:4]\n",
    "            p2 = out[0]\n",
    "            q2 = out[1]\n",
    "            r2 = out[2]\n",
    "            s2 = out[3]\n",
    "            \n",
    "            p = p0 + dt * (p1 + p2)/2\n",
    "            q = q0 + dt * (q1 + q2)/2\n",
    "            r = r0 + dt * (r1 + r2)/2\n",
    "            s = s0 + dt * (s1 + s2)/2\n",
    "\n",
    "            return [p, q, r, s] + x[4:]        \n",
    "        \n",
    "        p0 = x[0][:,0]\n",
    "        q0 = x[0][:,1]\n",
    "        out = dynamics(x, f)[0]\n",
    "        p1 = out[:, 0]\n",
    "        q1 = out[:, 1]\n",
    "\n",
    "        out = dynamics([torch.stack([p0 + 0.5 * dt * p1, q0 + 0.5 * dt * q1], dim = 1)], f)[0]\n",
    "        p2 = out[:, 0]\n",
    "        q2 = out[:, 1]\n",
    "\n",
    "        p = p0 + dt * (p1 + p2)/2\n",
    "        q = q0 + dt * (q1 + q2)/2\n",
    "        return [torch.stack([p, q], dim = 1)]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def heuns_method(x, dynamics, f, dt):\n",
    "        # Heun's method\n",
    "        # k1 = func(y)\n",
    "        # k2 = func(y + 0.5 * h * k1)\n",
    "        # return y + h * k2\n",
    "        k1_ = dynamics(x, f)\n",
    "        k2__ = [x_ + 0.5 * dt * k1 for x_, k1 in zip(x, k1_)]\n",
    "        k2_ = dynamics(k2__, f)\n",
    "        result = [x_ + 0.5 * dt * k2 for x_, k2 in zip(x, k2_)]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def predictor_corrector(x, dynamics, f, dt, iter):\n",
    "        # print(\"inside predictor corrector\")\n",
    "        def heuns_method(x, dynamics, f, dt):\n",
    "            k1_ = dynamics(x, f)\n",
    "            k2__ = [x_ + 0.5 * dt * k1 for x_, k1 in zip(x, k1_)]\n",
    "            k2_ = dynamics(k2__, f)\n",
    "            result = [x_ + 0.5 * dt * k2 for x_, k2 in zip(x, k2_)]\n",
    "            return result\n",
    "\n",
    "        #corrector\n",
    "        def verlet_implicit(x, dynamics, f, dt, predict):\n",
    "            if len(x) == 1:\n",
    "                p0 = x[0][:, 0]\n",
    "                q0 = x[0][:, 1]\n",
    "                p_half = predict[0][:, 0]\n",
    "                q = predict[0][:, 1]\n",
    "                p_half = p0 + 0.5 * dt * dynamics([torch.stack([p_half, q0], dim = 1)], f)[0][:, 0]\n",
    "                q = q0 + 0.5 * dt * (dynamics([torch.stack([p_half, q], dim = 1)], f)[0][:, 1] + dynamics([torch.stack([p_half, q0], dim = 1)], f)[0][:, 1])\n",
    "                p = p_half + 0.5 * dt * dynamics([torch.stack([p_half, q], dim = 1)], f)[0][:, 0]\n",
    "                return [torch.stack([p, q], dim = 1)]\n",
    "            else:\n",
    "                # print(\"should be here\")\n",
    "                p0 = x[0]\n",
    "                q0 = x[2]\n",
    "                p_half = predict[0]\n",
    "                q = predict[2]\n",
    "                p_half = p0 + 0.5 * dt * dynamics([p_half, x[1], q0, x[3]] + x[4:], f)[0]\n",
    "                q = q0 + 0.5 * dt * (dynamics([p_half, x[1], q, x[3]] + x[4:], f)[2] + dynamics([p_half, x[1], q0, x[3]] + x[4:], f)[2])\n",
    "                p = p_half + 0.5 * dt * dynamics([p_half, x[1], q, x[3]] + x[4:], f)[0]\n",
    "                p0_ = x[3]\n",
    "                q0_ = x[1]\n",
    "                p_half_ = predict[3]\n",
    "                q_ = predict[1]\n",
    "                p_half_ = p0_ + 0.5 * dt * dynamics([x[0], q0_, x[2], p_half_] + x[4:], f)[3]\n",
    "                q_ = q0_ + 0.5 * dt * (dynamics([x[0], q_, x[2], p_half_] + x[4:], f)[1] + dynamics([x[0], q0_, x[2], p_half_] + x[4:], f)[1])\n",
    "                p_ = p_half_ + 0.5 * dt * dynamics([x[0], q_, x[2], p_half_] + x[4:], f)[3]\n",
    "                state = [p, q_, q, p_] + x[4:]\n",
    "                # res = euler_step(state, adjoint_param_dynamics, f, dt)\n",
    "                return [p, q_, q, p_]\n",
    "        x_ = x\n",
    "        for _ in range(iter):\n",
    "            predict = heuns_method(x_[:4], dynamics, f, dt)\n",
    "            result = verlet_implicit(x_[:4], dynamics, f, dt, predict)\n",
    "            x_ = result + x_[4:]\n",
    "        return x_\n",
    "\n",
    "    @staticmethod\n",
    "    def dynamics_fn(x, f):\n",
    "        x_ = x[0]\n",
    "        p = x_[:, 0]\n",
    "        q = x_[:, 1]\n",
    "        with torch.enable_grad():\n",
    "            p.requires_grad_(True)\n",
    "            q.requires_grad_(True)\n",
    "            h = f(p.unsqueeze(-1), q.unsqueeze(-1))\n",
    "            grad_p, = grad(h.sum(), p, create_graph=True, allow_unused=True)\n",
    "            grad_q, = grad(h.sum(), q, create_graph=True, allow_unused=True)\n",
    "            return [torch.stack([-grad_q, grad_p], dim = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cd77f-b26a-421d-8b9e-c94c68c804a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hnn_adjoint(func, x, T, dt, back_dt, dim, integrator, back_integrator, iter):\n",
    "    adjoint_params = tuple(list(func.parameters()))\n",
    "    adjoint_params = tuple(p for p in adjoint_params if p.requires_grad)\n",
    "    trajectories = Hamiltonian_Adjoint.apply(x[:, 0], x[:, 1], func, T, dt, back_dt, dim, integrator, back_integrator, iter, *adjoint_params)\n",
    "    return trajectories[:, :, 0], trajectories[:, :, 1]\n",
    "\n",
    "class Symplectic_HNN(nn.Module):\n",
    "    def __init__(self, f, T = 30, dt = 0.1, back_dt = 0.1, integrator = \"euler\", back_integrator = \"euler\", dim = 1, iter = 1):\n",
    "        super(Symplectic_HNN, self).__init__()\n",
    "        self.func = f\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.back_dt = back_dt\n",
    "        self.dim = dim\n",
    "        self.integrator = integrator\n",
    "        self.back_integrator = back_integrator\n",
    "        self.iter = iter\n",
    "\n",
    "    def forward(self, p0, q0):\n",
    "        p, q = hnn_adjoint(self.func, torch.stack([p0, q0], dim = 1), self.T, self.dt, self.back_dt, self.dim, self.integrator, self.back_integrator, self.iter)\n",
    "        p = torch.unsqueeze(p, dim = -1)\n",
    "        q = torch.unsqueeze(q, dim = -1)\n",
    "        return torch.cat([p, q], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba1674-5d8a-49f3-8f18-d91b6d740fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for integrator in [\"euler\"]:\n",
    "    for back_integrator in [\"euler\", \"rk2\", \"sv\"]:\n",
    "\n",
    "        f1 = MLP_General_Hamilt(n_input = 1, n_hidden = 64)\n",
    "        # f1 = MLP2H_Separable_Hamilt(n_hidden=256, input_size=1).to(device).double()\n",
    "        T = 2\n",
    "        dt = 0.1\n",
    "        back_dt = 0.1\n",
    "        dim = 1\n",
    "        num_epochs = 100\n",
    "        iter = 1\n",
    "        # integrator = \"euler\"\n",
    "        # back_integrator = \"euler\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Create the model, criterion, optimizer, and data loaders\n",
    "        model = Symplectic_HNN(f1, T, dt, back_dt, integrator, back_integrator, dim, iter).to(device).double()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "        train_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/train.npy\")\n",
    "        data_loader = DataLoader(train_mass_spring, batch_size=32, shuffle=False)\n",
    "        val_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/val.npy\")\n",
    "        val_loader = DataLoader(val_mass_spring, batch_size=32, shuffle=False)\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_path = integrator+\"_\"+back_integrator+'_best_model.pt'\n",
    "\n",
    "        with open(integrator+\"_\"+back_integrator+\"_loss_adjoint_\"+str(dt)+\".txt\", \"w\") as loss_file:\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                model.train()\n",
    "                loss_epoch = []\n",
    "                for batch in data_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    p_batch, q_batch, _ = batch\n",
    "                    p0_batch = p_batch[:, 0].to(device).double()\n",
    "                    q0_batch = q_batch[:, 1].to(device).double()\n",
    "                    simulated_trajectory = model(p0_batch, q0_batch)\n",
    "                    simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "                    trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "                    trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "                    loss = criterion(trajectory, simulated_trajectory)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    loss_epoch.append(loss.item())\n",
    "                avg_train_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "                train_loss.append(avg_train_loss)\n",
    "\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                val_loss_epoch = []\n",
    "                for batch in val_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    p_batch, q_batch, _ = batch\n",
    "                    p0_batch = p_batch[:, 0].to(device).double()\n",
    "                    q0_batch = q_batch[:, 1].to(device).double()\n",
    "                    simulated_trajectory = model(p0_batch, q0_batch)\n",
    "                    simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "                    trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "                    trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "                    loss = criterion(trajectory, simulated_trajectory)\n",
    "                    val_loss_epoch.append(loss.item())\n",
    "                avg_val_loss = sum(val_loss_epoch)/len(val_loss_epoch)\n",
    "                val_loss.append(avg_val_loss)\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "                loss_file.write(f\"{avg_train_loss}, {avg_val_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268111cb-3abf-43ed-bc8a-5eb5d84f8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read loss from a file\n",
    "integrator+\"_loss_\"+str(dt)+\".txt\"\n",
    "import os\n",
    "def read_loss(file_path):\n",
    "    val_losses = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            train_loss, val_loss = map(float, line.strip().split(','))\n",
    "            val_losses.append(val_loss)\n",
    "    return val_losses\n",
    "\n",
    "# List of file paths for different variants\n",
    "file_paths = [\"euler_loss_0.1.txt\", \"euler_euler_loss_adjoint_0.1.txt\", \"euler_rk2_loss_adjoint_0.1.txt\"]\n",
    "\n",
    "# Initialize a list to store validation loss trajectories for each variant\n",
    "all_val_losses = []\n",
    "\n",
    "# Read validation losses for each variant\n",
    "for file_path in file_paths:\n",
    "    val_losses = read_loss(file_path)\n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "# Plot the validation loss trajectories for each variant\n",
    "epochs = range(1, len(all_val_losses[0]) + 1)\n",
    "for file_path, val_losses in zip(file_paths, all_val_losses):\n",
    "    variant_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    plt.plot(epochs, val_losses, label=variant_name)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Trajectories for Different Variants')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "4945b87c-a256-4992-9f90-a31a7b530b21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the parameters equal between f1 and f2? True\n"
     ]
    }
   ],
   "source": [
    "# f1 = MLP2H_Separable_Hamilt(n_hidden=256, input_size=1).to(device).double()\n",
    "# # Clone f1 to create f2\n",
    "# f2 = MLP2H_Separable_Hamilt(n_hidden=256, input_size=1).to(device).double()\n",
    "\n",
    "f1 = MLP_General_Hamilt(n_input = 1, n_hidden = 64)\n",
    "f2 = MLP_General_Hamilt(n_input = 1, n_hidden = 64)\n",
    "# Copy parameters from f1 to f2\n",
    "f2.load_state_dict(f1.state_dict())\n",
    "\n",
    "params_equal = all(torch.allclose(p1, p2) for p1, p2 in zip(f1.parameters(), f2.parameters()))\n",
    "print(\"Are the parameters equal between f1 and f2?\", params_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "f35ac578-7d39-4961-852f-6fb7700265d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:07<01:04,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Train Loss: 7.482466161251068, Average Val Loss: 6.898046697889056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:13<00:54,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Train Loss: 7.462766423821449, Average Val Loss: 6.818754468645368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [00:20<00:46,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Train Loss: 6.979049831628799, Average Val Loss: 5.834603990827288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [00:26<00:39,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Train Loss: 6.016539961099625, Average Val Loss: 5.577141012464251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [00:32<00:32,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Train Loss: 5.93646177649498, Average Val Loss: 5.548423971448626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [00:39<00:26,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Train Loss: 5.892290145158768, Average Val Loss: 5.542686189923968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████▊             | 7/10 [00:46<00:20,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Train Loss: 5.883467882871628, Average Val Loss: 5.540743282863072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 8/10 [00:53<00:13,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Train Loss: 5.873905703425407, Average Val Loss: 5.537671497889927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [01:01<00:07,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Train Loss: 5.858141779899597, Average Val Loss: 5.517237731388637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:10<00:00,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Train Loss: 5.837831005454063, Average Val Loss: 5.494555677686419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "T = 2\n",
    "dt = 0.1\n",
    "dim = 1\n",
    "num_epochs = 10\n",
    "iter = 1\n",
    "integrator = \"rk2\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# f = MLP2H_Separable_Hamilt(n_hidden = 256, input_size = 1).to(device)\n",
    "model = BackProp_HNN(f1, T, dt, dim, integrator, iter).to(device).double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/train.npy\")\n",
    "data_loader = DataLoader(train_mass_spring, batch_size=32, shuffle=False)\n",
    "val_mass_spring = HarmonicOscillatorDataset(\"./data/mass_spring/val.npy\")\n",
    "val_loader = DataLoader(val_mass_spring, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = integrator+'_best_model.pt'\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    loss_epoch = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        p_batch, q_batch, _ = batch\n",
    "        p0_batch = p_batch[:, 0].to(device).double()\n",
    "        q0_batch = q_batch[:, 1].to(device).double()\n",
    "        simulated_trajectory = model(p0_batch, q0_batch)\n",
    "        # print(\"simulated trajectory\", simulated_trajectory[0, :, 0])\n",
    "        simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "        trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "        trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "        loss = criterion(trajectory, simulated_trajectory)\n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # for name, param in f1.named_parameters():\n",
    "        #     # if param.grad\n",
    "        #     print(name)\n",
    "        #     print(param.grad)\n",
    "        loss_epoch.append(loss.item())\n",
    "    avg_train_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "    # print(\"avg train loss\", avg_train_loss)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss_epoch = []\n",
    "    # with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        optimizer.zero_grad()\n",
    "        p_batch, q_batch, _ = batch\n",
    "        p0_batch = p_batch[:, 0].to(device).double()\n",
    "        q0_batch = q_batch[:, 1].to(device).double()\n",
    "        simulated_trajectory = model(p0_batch, q0_batch)\n",
    "        simulated_trajectory = simulated_trajectory.view(simulated_trajectory.size(0), -1)\n",
    "        trajectory = torch.stack([p_batch, q_batch], axis=2).to(device)\n",
    "        trajectory = trajectory.view(trajectory.size(0), -1).to(device)\n",
    "        loss = criterion(trajectory, simulated_trajectory)\n",
    "        val_loss_epoch.append(loss.item())\n",
    "    avg_val_loss = sum(val_loss_epoch)/len(val_loss_epoch)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    # Check if the validation loss is the lowest seen so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    print(f\"Epoch {epoch+1}, Average Train Loss: {avg_train_loss}, Average Val Loss: {avg_val_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b8050ba7-d61e-4190-bd9c-6eb65e1376ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add solvers\n",
    "'''\n",
    "def euler(func, p0, q0, dt):\n",
    "    dp_dt, dq_dt = func(p0, q0)\n",
    "    p = p0 + dt * dp_dt\n",
    "    q = q0 + dt * dq_dt\n",
    "    return p, q\n",
    "\n",
    "def rk2(func, p0, q0, dt):\n",
    "    pass\n",
    "\n",
    "def sv(func, p0, q0, dt, iterations = 1, p_init = None, q_init = None):\n",
    "    pass\n",
    "\n",
    "def pc(func, p0, q0, dt, iterations = 1):\n",
    "    p_init, q_init = rk2(func, p0, q0, dt)\n",
    "    p, q = sv(func, p0, q0, T, dt, iterations, p_init, q_init)\n",
    "    return p, q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7ac6758c-a149-4ffc-bcc7-1ffdd0077120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Linear(inchannel, outchannel),\n",
    "            #nn.Tanh(),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        return out\n",
    "\n",
    "class NN_Backprop(nn.Module):\n",
    "    def __init__(self, N, hidden_dim):\n",
    "        super(NN_Backprop, self).__init__()\n",
    "        self.N = N\n",
    "        self.f = nn.Sequential(LinearBlock(2 * self.N, hidden_dim),\n",
    "                                    LinearBlock(hidden_dim, hidden_dim),\n",
    "                                    LinearBlock(hidden_dim, hidden_dim),\n",
    "                                    LinearBlock(hidden_dim, hidden_dim),\n",
    "                                    LinearBlock(hidden_dim, hidden_dim),\n",
    "                                    nn.Linear(hidden_dim, 2*self.N))\n",
    "        self.b = nn.Parameter(torch.zeros(1,1,2*self.N) , requires_grad=True)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.uniform_(-math.sqrt(6. / m.in_features), math.sqrt(6. / m.in_features))\n",
    "                \n",
    "    def hamiltonian(self, p, q):\n",
    "        # with torch.enable_grad():\n",
    "        x = torch.cat((p.unsqueeze(1), q.unsqueeze(1)), dim=1)\n",
    "        x = x.requires_grad_(True)\n",
    "        K = self.f(x)+self.b\n",
    "        return K[:, :, :self.N] + K[:, :, self.N:self.N * 2]\n",
    "    \n",
    "    def dynamics(self, p, q):\n",
    "        p.requires_grad_(True)\n",
    "        q.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            h = self.hamiltonian(p, q)\n",
    "            print(h.requires_grad_)\n",
    "            grad_p, = grad(h.sum(), p, create_graph=True, allow_unused=True)\n",
    "            grad_q, = grad(h.sum(), q, create_graph=True, allow_unused=True)\n",
    "        return -grad_q, grad_p\n",
    "        \n",
    "    def forward(self, p0, q0, solver, T, dt):\n",
    "        trajectory = torch.zeros((p0.shape[0], T, self.N * 2)).to(device)\n",
    "        n_steps = int(np.ceil(T/dt))\n",
    "        trajectory[:, 0, 0] = p0\n",
    "        trajectory[:, 0, 1] = q0\n",
    "        p = p0\n",
    "        q = q0\n",
    "        for i in range(1, T):\n",
    "            for _ in range(n_steps):\n",
    "                p, q = solver(self.dynamics, p, q, dt)\n",
    "            trajectory[:, i, 0] = p\n",
    "            trajectory[:, i, 1] = q\n",
    "        return trajectory[:, :, 0], trajectory[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "99107e1f-b9b4-4851-8131-d5e991c585be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hamiltonian_Data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Load the dataset\n",
    "        dataset = np.load(file_path)[:, ::1000, :]\n",
    "        \n",
    "        # Convert the dataset to PyTorch tensors\n",
    "        self.p_values = torch.tensor(dataset[:, :, 0], dtype=torch.float32)\n",
    "        self.q_values = torch.tensor(dataset[:, :, 1], dtype=torch.float32)\n",
    "        self.h_values = torch.tensor(dataset[:, :, 2], dtype=torch.float32)\n",
    "        \n",
    "        # Ensure consistent length for all tensors\n",
    "        assert len(self.p_values) == len(self.q_values) == len(self.h_values)\n",
    "        self.length = len(self.p_values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.p_values[idx], self.q_values[idx], self.h_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a2c97307-21d9-498d-8a97-28f5ff8b0100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mass_spring = Hamiltonian_Data(\"./data/mass_spring/train.npy\")\n",
    "train_loader = DataLoader(train_mass_spring, batch_size=32, shuffle=True)\n",
    "val_mass_spring = Hamiltonian_Data(\"./data/mass_spring/val.npy\")\n",
    "val_loader = DataLoader(val_mass_spring, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "28ed8876-c9ff-48b8-9e83-8a8fdc835279",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "p_values: torch.Size([32, 10])\n",
      "q_values: torch.Size([32, 10])\n",
      "h_values: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (p_values, q_values, h_values) in enumerate(train_loader):\n",
    "    # Perform operations on the batch\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"p_values:\", p_values.shape)\n",
    "    print(\"q_values:\", q_values.shape)\n",
    "    print(\"h_values:\", h_values.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c3f9f70d-a546-4ef8-8d46-8f55dcb05384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m NN_Backprop(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrain_and_validate_with_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss.log\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[218], line 26\u001b[0m, in \u001b[0;36mtrain_and_validate_with_logging\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, checkpoint_path, log_file, T, dt)\u001b[0m\n\u001b[1;32m     23\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/PINNs/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PINNs/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Define the Hamiltonian model\n",
    "# (same as before)\n",
    "\n",
    "# Define the training loop with validation and loss logging\n",
    "def train_and_validate_with_logging(model, train_loader, val_loader, optimizer, criterion, epochs=10, checkpoint_path='checkpoint.pt', log_file='loss.log', T = 10, dt = 0.1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for batch_idx, (p_values, q_values, h_values) in enumerate(train_loader):\n",
    "            p_values, q_values, h_values = p_values.to(device), q_values.to(device), h_values.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            p_pred, q_pred = model(p_values[:, 0], q_values[:, 0], euler, T, dt)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(p_pred, p_values) + criterion(q_pred, q_values)\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#         epoch_train_loss /= len(train_loader)\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         epoch_val_loss = 0.0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for p_values, q_values, h_values in val_loader:\n",
    "#                 p_values, q_values, h_values = p_values.to(device), q_values.to(device), h_values.to(device)\n",
    "#                 p_pred, q_pred = model(p_values[:, 0], q_values[:, 0], euler, T, dt)\n",
    "#                 val_loss = criterion(p_pred, p_values) + criterion(q_pred, q_values)\n",
    "#                 epoch_val_loss += val_loss.item()\n",
    "\n",
    "#         epoch_val_loss /= len(val_loader)\n",
    "\n",
    "#         print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "#         # Save losses to list\n",
    "#         losses.append((epoch_train_loss, epoch_val_loss))\n",
    "\n",
    "#         # Save checkpoint if validation loss has decreased\n",
    "#         if epoch_val_loss < best_val_loss:\n",
    "#             best_val_loss = epoch_val_loss\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "#             print(\"Checkpoint saved.\")\n",
    "\n",
    "#     # Write losses to file\n",
    "#     with open(log_file, 'w') as f:\n",
    "#         f.write(\"Epoch\\tTrain Loss\\tVal Loss\\n\")\n",
    "#         for i, (train_loss, val_loss) in enumerate(losses):\n",
    "#             f.write(f\"{i+1}\\t{train_loss:.6f}\\t{val_loss:.6f}\\n\")\n",
    "\n",
    "# Define hyperparameters and instantiate the model and datasets\n",
    "# (same as before)\n",
    "\n",
    "# Train, validate, and log the model\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 1\n",
    "model = NN_Backprop(1, 64)\n",
    "train_and_validate_with_logging(model, train_loader, val_loader, optimizer, criterion, epochs, 'checkpoint.pt', 'loss.log', 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70595c6-e359-4384-86e3-26c783dd5ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PINNs)",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
